{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "DIR = \"/Users/rithikpothuganti/quantsc/Sports-Betting\"\n",
    "START_YEAR = 1990\n",
    "END_YEAR = 2012\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TEAM</th>\n",
       "      <th>Age</th>\n",
       "      <th>W</th>\n",
       "      <th>L</th>\n",
       "      <th>PW</th>\n",
       "      <th>PL</th>\n",
       "      <th>MOV</th>\n",
       "      <th>SOS</th>\n",
       "      <th>SRS</th>\n",
       "      <th>ORtg</th>\n",
       "      <th>...</th>\n",
       "      <th>opp_ORB</th>\n",
       "      <th>opp_DRB</th>\n",
       "      <th>opp_TRB</th>\n",
       "      <th>opp_AST</th>\n",
       "      <th>opp_STL</th>\n",
       "      <th>opp_BLK</th>\n",
       "      <th>opp_TOV</th>\n",
       "      <th>opp_PF</th>\n",
       "      <th>opp_PTS</th>\n",
       "      <th>PLAYOFF_WINS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LAL_1990</td>\n",
       "      <td>28.9</td>\n",
       "      <td>63.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>58</td>\n",
       "      <td>24</td>\n",
       "      <td>6.78</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>6.74</td>\n",
       "      <td>114.0</td>\n",
       "      <td>...</td>\n",
       "      <td>14.1</td>\n",
       "      <td>29.5</td>\n",
       "      <td>43.5</td>\n",
       "      <td>24.1</td>\n",
       "      <td>9.5</td>\n",
       "      <td>4.7</td>\n",
       "      <td>17.1</td>\n",
       "      <td>21.8</td>\n",
       "      <td>103.4</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PHO_1990</td>\n",
       "      <td>26.7</td>\n",
       "      <td>54.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>58</td>\n",
       "      <td>24</td>\n",
       "      <td>7.10</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>7.09</td>\n",
       "      <td>113.1</td>\n",
       "      <td>...</td>\n",
       "      <td>13.4</td>\n",
       "      <td>29.3</td>\n",
       "      <td>42.6</td>\n",
       "      <td>22.7</td>\n",
       "      <td>7.8</td>\n",
       "      <td>3.9</td>\n",
       "      <td>16.0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>103.5</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DET_1990</td>\n",
       "      <td>29.5</td>\n",
       "      <td>59.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>57</td>\n",
       "      <td>25</td>\n",
       "      <td>6.09</td>\n",
       "      <td>-0.68</td>\n",
       "      <td>5.41</td>\n",
       "      <td>109.9</td>\n",
       "      <td>...</td>\n",
       "      <td>13.9</td>\n",
       "      <td>27.7</td>\n",
       "      <td>41.6</td>\n",
       "      <td>24.3</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5.1</td>\n",
       "      <td>17.5</td>\n",
       "      <td>23.7</td>\n",
       "      <td>104.2</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>POR_1990</td>\n",
       "      <td>26.7</td>\n",
       "      <td>59.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>57</td>\n",
       "      <td>25</td>\n",
       "      <td>6.32</td>\n",
       "      <td>0.16</td>\n",
       "      <td>6.48</td>\n",
       "      <td>110.5</td>\n",
       "      <td>...</td>\n",
       "      <td>12.5</td>\n",
       "      <td>27.8</td>\n",
       "      <td>40.3</td>\n",
       "      <td>23.9</td>\n",
       "      <td>7.9</td>\n",
       "      <td>5.1</td>\n",
       "      <td>17.6</td>\n",
       "      <td>24.6</td>\n",
       "      <td>104.4</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PHI_1990</td>\n",
       "      <td>27.4</td>\n",
       "      <td>53.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>54</td>\n",
       "      <td>28</td>\n",
       "      <td>4.99</td>\n",
       "      <td>-0.75</td>\n",
       "      <td>4.23</td>\n",
       "      <td>113.5</td>\n",
       "      <td>...</td>\n",
       "      <td>13.9</td>\n",
       "      <td>27.8</td>\n",
       "      <td>41.7</td>\n",
       "      <td>23.7</td>\n",
       "      <td>9.3</td>\n",
       "      <td>5.7</td>\n",
       "      <td>15.6</td>\n",
       "      <td>24.1</td>\n",
       "      <td>105.4</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>628</th>\n",
       "      <td>NJN_2011</td>\n",
       "      <td>24.8</td>\n",
       "      <td>24.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>24</td>\n",
       "      <td>58</td>\n",
       "      <td>-6.24</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-6.28</td>\n",
       "      <td>103.1</td>\n",
       "      <td>...</td>\n",
       "      <td>13.4</td>\n",
       "      <td>33.5</td>\n",
       "      <td>46.9</td>\n",
       "      <td>25.9</td>\n",
       "      <td>8.6</td>\n",
       "      <td>4.5</td>\n",
       "      <td>16.9</td>\n",
       "      <td>19.3</td>\n",
       "      <td>110.7</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>629</th>\n",
       "      <td>TOR_2011</td>\n",
       "      <td>24.5</td>\n",
       "      <td>22.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>24</td>\n",
       "      <td>58</td>\n",
       "      <td>-6.28</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-6.28</td>\n",
       "      <td>106.1</td>\n",
       "      <td>...</td>\n",
       "      <td>11.5</td>\n",
       "      <td>32.4</td>\n",
       "      <td>43.9</td>\n",
       "      <td>25.5</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5.6</td>\n",
       "      <td>14.9</td>\n",
       "      <td>21.6</td>\n",
       "      <td>111.1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>630</th>\n",
       "      <td>MIN_2011</td>\n",
       "      <td>23.8</td>\n",
       "      <td>17.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>24</td>\n",
       "      <td>58</td>\n",
       "      <td>-6.63</td>\n",
       "      <td>0.67</td>\n",
       "      <td>-5.97</td>\n",
       "      <td>104.2</td>\n",
       "      <td>...</td>\n",
       "      <td>11.7</td>\n",
       "      <td>33.4</td>\n",
       "      <td>45.2</td>\n",
       "      <td>25.9</td>\n",
       "      <td>7.7</td>\n",
       "      <td>5.2</td>\n",
       "      <td>15.7</td>\n",
       "      <td>20.9</td>\n",
       "      <td>111.7</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>631</th>\n",
       "      <td>WAS_2011</td>\n",
       "      <td>24.9</td>\n",
       "      <td>23.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>22</td>\n",
       "      <td>60</td>\n",
       "      <td>-7.40</td>\n",
       "      <td>0.11</td>\n",
       "      <td>-7.30</td>\n",
       "      <td>102.4</td>\n",
       "      <td>...</td>\n",
       "      <td>11.7</td>\n",
       "      <td>34.9</td>\n",
       "      <td>46.6</td>\n",
       "      <td>25.9</td>\n",
       "      <td>8.1</td>\n",
       "      <td>6.4</td>\n",
       "      <td>14.5</td>\n",
       "      <td>22.8</td>\n",
       "      <td>111.8</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>632</th>\n",
       "      <td>CLE_2011</td>\n",
       "      <td>26.6</td>\n",
       "      <td>19.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>18</td>\n",
       "      <td>64</td>\n",
       "      <td>-9.01</td>\n",
       "      <td>0.14</td>\n",
       "      <td>-8.88</td>\n",
       "      <td>102.2</td>\n",
       "      <td>...</td>\n",
       "      <td>11.9</td>\n",
       "      <td>32.1</td>\n",
       "      <td>44.0</td>\n",
       "      <td>24.2</td>\n",
       "      <td>8.2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>14.9</td>\n",
       "      <td>21.2</td>\n",
       "      <td>112.9</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>633 rows × 72 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         TEAM   Age     W     L  PW  PL   MOV   SOS   SRS   ORtg  ...  \\\n",
       "0    LAL_1990  28.9  63.0  19.0  58  24  6.78 -0.04  6.74  114.0  ...   \n",
       "1    PHO_1990  26.7  54.0  28.0  58  24  7.10 -0.01  7.09  113.1  ...   \n",
       "2    DET_1990  29.5  59.0  23.0  57  25  6.09 -0.68  5.41  109.9  ...   \n",
       "3    POR_1990  26.7  59.0  23.0  57  25  6.32  0.16  6.48  110.5  ...   \n",
       "4    PHI_1990  27.4  53.0  29.0  54  28  4.99 -0.75  4.23  113.5  ...   \n",
       "..        ...   ...   ...   ...  ..  ..   ...   ...   ...    ...  ...   \n",
       "628  NJN_2011  24.8  24.0  58.0  24  58 -6.24 -0.04 -6.28  103.1  ...   \n",
       "629  TOR_2011  24.5  22.0  60.0  24  58 -6.28  0.01 -6.28  106.1  ...   \n",
       "630  MIN_2011  23.8  17.0  65.0  24  58 -6.63  0.67 -5.97  104.2  ...   \n",
       "631  WAS_2011  24.9  23.0  59.0  22  60 -7.40  0.11 -7.30  102.4  ...   \n",
       "632  CLE_2011  26.6  19.0  63.0  18  64 -9.01  0.14 -8.88  102.2  ...   \n",
       "\n",
       "     opp_ORB  opp_DRB  opp_TRB  opp_AST  opp_STL  opp_BLK  opp_TOV  opp_PF  \\\n",
       "0       14.1     29.5     43.5     24.1      9.5      4.7     17.1    21.8   \n",
       "1       13.4     29.3     42.6     22.7      7.8      3.9     16.0    26.6   \n",
       "2       13.9     27.7     41.6     24.3      9.4      5.1     17.5    23.7   \n",
       "3       12.5     27.8     40.3     23.9      7.9      5.1     17.6    24.6   \n",
       "4       13.9     27.8     41.7     23.7      9.3      5.7     15.6    24.1   \n",
       "..       ...      ...      ...      ...      ...      ...      ...     ...   \n",
       "628     13.4     33.5     46.9     25.9      8.6      4.5     16.9    19.3   \n",
       "629     11.5     32.4     43.9     25.5      9.8      5.6     14.9    21.6   \n",
       "630     11.7     33.4     45.2     25.9      7.7      5.2     15.7    20.9   \n",
       "631     11.7     34.9     46.6     25.9      8.1      6.4     14.5    22.8   \n",
       "632     11.9     32.1     44.0     24.2      8.2      6.0     14.9    21.2   \n",
       "\n",
       "     opp_PTS  PLAYOFF_WINS  \n",
       "0      103.4           4.0  \n",
       "1      103.5           9.0  \n",
       "2      104.2          15.0  \n",
       "3      104.4          12.0  \n",
       "4      105.4           4.0  \n",
       "..       ...           ...  \n",
       "628    110.7          -1.0  \n",
       "629    111.1          -1.0  \n",
       "630    111.7          -1.0  \n",
       "631    111.8          -1.0  \n",
       "632    112.9          -1.0  \n",
       "\n",
       "[633 rows x 72 columns]"
      ]
     },
     "execution_count": 501,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "team_stats = pd.read_csv(f\"{DIR}/data/NBA_team_stats/{START_YEAR}/team_stats.csv\", usecols=lambda column: column.startswith('Unnamed') == False)\n",
    "playoff_wins = pd.read_csv(f\"{DIR}/data/NBA_playoff_wins/{START_YEAR}/playoff_wins.csv\", usecols=lambda column: column.startswith('Unnamed') == False)\n",
    "for year in range(START_YEAR+1, END_YEAR):\n",
    "    playoff_wins_curr = pd.read_csv(f\"{DIR}/data/NBA_playoff_wins/{year}/playoff_wins.csv\", usecols=lambda column: column.startswith('Unnamed') == False)\n",
    "    playoff_wins = pd.concat([playoff_wins, playoff_wins_curr])\n",
    "    team_stats_curr = pd.read_csv(f\"{DIR}/data/NBA_team_stats/{year}/team_stats.csv\", usecols=lambda column: column.startswith('Unnamed') == False)\n",
    "    team_stats = pd.concat([team_stats, team_stats_curr])\n",
    "\n",
    "data = pd.merge(team_stats, playoff_wins, how='left').fillna(-1)\n",
    "\n",
    "data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>W</th>\n",
       "      <th>L</th>\n",
       "      <th>PW</th>\n",
       "      <th>PL</th>\n",
       "      <th>MOV</th>\n",
       "      <th>SOS</th>\n",
       "      <th>SRS</th>\n",
       "      <th>ORtg</th>\n",
       "      <th>DRtg</th>\n",
       "      <th>...</th>\n",
       "      <th>opp_FT%</th>\n",
       "      <th>opp_ORB</th>\n",
       "      <th>opp_DRB</th>\n",
       "      <th>opp_TRB</th>\n",
       "      <th>opp_AST</th>\n",
       "      <th>opp_STL</th>\n",
       "      <th>opp_BLK</th>\n",
       "      <th>opp_TOV</th>\n",
       "      <th>opp_PF</th>\n",
       "      <th>opp_PTS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.623435</td>\n",
       "      <td>-0.289593</td>\n",
       "      <td>-1.697278</td>\n",
       "      <td>-0.422940</td>\n",
       "      <td>-1.528993</td>\n",
       "      <td>-0.505834</td>\n",
       "      <td>-1.041598</td>\n",
       "      <td>-0.490948</td>\n",
       "      <td>-0.326051</td>\n",
       "      <td>-0.915459</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.179457</td>\n",
       "      <td>-0.759467</td>\n",
       "      <td>-1.485863</td>\n",
       "      <td>-1.273512</td>\n",
       "      <td>-0.964155</td>\n",
       "      <td>-0.623357</td>\n",
       "      <td>-1.291822</td>\n",
       "      <td>-0.819774</td>\n",
       "      <td>-1.377670</td>\n",
       "      <td>-1.264973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.096553</td>\n",
       "      <td>-0.570843</td>\n",
       "      <td>-1.402196</td>\n",
       "      <td>-0.422940</td>\n",
       "      <td>-1.528993</td>\n",
       "      <td>-0.482511</td>\n",
       "      <td>-1.010510</td>\n",
       "      <td>-0.464513</td>\n",
       "      <td>-0.403305</td>\n",
       "      <td>-0.993129</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.794136</td>\n",
       "      <td>-0.959467</td>\n",
       "      <td>-1.526679</td>\n",
       "      <td>-1.423512</td>\n",
       "      <td>-1.205535</td>\n",
       "      <td>-1.264866</td>\n",
       "      <td>-1.632248</td>\n",
       "      <td>-1.048940</td>\n",
       "      <td>-0.480474</td>\n",
       "      <td>-1.255265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.494403</td>\n",
       "      <td>-0.414593</td>\n",
       "      <td>-1.566131</td>\n",
       "      <td>-0.455727</td>\n",
       "      <td>-1.496206</td>\n",
       "      <td>-0.556126</td>\n",
       "      <td>-1.704811</td>\n",
       "      <td>-0.591402</td>\n",
       "      <td>-0.677983</td>\n",
       "      <td>-1.255265</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.702393</td>\n",
       "      <td>-0.816610</td>\n",
       "      <td>-1.853210</td>\n",
       "      <td>-1.590179</td>\n",
       "      <td>-0.929673</td>\n",
       "      <td>-0.661093</td>\n",
       "      <td>-1.121609</td>\n",
       "      <td>-0.736440</td>\n",
       "      <td>-1.022530</td>\n",
       "      <td>-1.187303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.096553</td>\n",
       "      <td>-0.414593</td>\n",
       "      <td>-1.566131</td>\n",
       "      <td>-0.455727</td>\n",
       "      <td>-1.496206</td>\n",
       "      <td>-0.539362</td>\n",
       "      <td>-0.834344</td>\n",
       "      <td>-0.510586</td>\n",
       "      <td>-0.626481</td>\n",
       "      <td>-1.167886</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.702393</td>\n",
       "      <td>-1.216610</td>\n",
       "      <td>-1.832801</td>\n",
       "      <td>-1.806846</td>\n",
       "      <td>-0.998638</td>\n",
       "      <td>-1.227130</td>\n",
       "      <td>-1.121609</td>\n",
       "      <td>-0.715607</td>\n",
       "      <td>-0.854306</td>\n",
       "      <td>-1.167886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.946016</td>\n",
       "      <td>-0.602093</td>\n",
       "      <td>-1.369409</td>\n",
       "      <td>-0.554088</td>\n",
       "      <td>-1.397845</td>\n",
       "      <td>-0.636301</td>\n",
       "      <td>-1.777349</td>\n",
       "      <td>-0.680526</td>\n",
       "      <td>-0.368970</td>\n",
       "      <td>-0.779536</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.812485</td>\n",
       "      <td>-0.816610</td>\n",
       "      <td>-1.832801</td>\n",
       "      <td>-1.573512</td>\n",
       "      <td>-1.033121</td>\n",
       "      <td>-0.698829</td>\n",
       "      <td>-0.866290</td>\n",
       "      <td>-1.132274</td>\n",
       "      <td>-0.947764</td>\n",
       "      <td>-1.070799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>628</th>\n",
       "      <td>-1.505156</td>\n",
       "      <td>-1.508343</td>\n",
       "      <td>-0.418590</td>\n",
       "      <td>-1.537695</td>\n",
       "      <td>-0.414239</td>\n",
       "      <td>-1.454814</td>\n",
       "      <td>-1.041598</td>\n",
       "      <td>-1.474332</td>\n",
       "      <td>-1.261674</td>\n",
       "      <td>-0.633905</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.537255</td>\n",
       "      <td>-0.959467</td>\n",
       "      <td>-0.669536</td>\n",
       "      <td>-0.706846</td>\n",
       "      <td>-0.653811</td>\n",
       "      <td>-0.962980</td>\n",
       "      <td>-1.376929</td>\n",
       "      <td>-0.861440</td>\n",
       "      <td>-1.844960</td>\n",
       "      <td>-0.556236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>629</th>\n",
       "      <td>-1.569672</td>\n",
       "      <td>-1.570843</td>\n",
       "      <td>-0.353016</td>\n",
       "      <td>-1.537695</td>\n",
       "      <td>-0.414239</td>\n",
       "      <td>-1.457729</td>\n",
       "      <td>-0.989785</td>\n",
       "      <td>-1.474332</td>\n",
       "      <td>-1.004163</td>\n",
       "      <td>-0.342643</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.500558</td>\n",
       "      <td>-1.502325</td>\n",
       "      <td>-0.894026</td>\n",
       "      <td>-1.206846</td>\n",
       "      <td>-0.722776</td>\n",
       "      <td>-0.510149</td>\n",
       "      <td>-0.908843</td>\n",
       "      <td>-1.278107</td>\n",
       "      <td>-1.415054</td>\n",
       "      <td>-0.517401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>630</th>\n",
       "      <td>-1.720209</td>\n",
       "      <td>-1.727093</td>\n",
       "      <td>-0.189081</td>\n",
       "      <td>-1.537695</td>\n",
       "      <td>-0.414239</td>\n",
       "      <td>-1.483240</td>\n",
       "      <td>-0.305847</td>\n",
       "      <td>-1.450918</td>\n",
       "      <td>-1.167253</td>\n",
       "      <td>-0.517401</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.720741</td>\n",
       "      <td>-1.445182</td>\n",
       "      <td>-0.689944</td>\n",
       "      <td>-0.990179</td>\n",
       "      <td>-0.653811</td>\n",
       "      <td>-1.302602</td>\n",
       "      <td>-1.079056</td>\n",
       "      <td>-1.111440</td>\n",
       "      <td>-1.545895</td>\n",
       "      <td>-0.459148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>631</th>\n",
       "      <td>-1.483650</td>\n",
       "      <td>-1.539593</td>\n",
       "      <td>-0.385803</td>\n",
       "      <td>-1.603268</td>\n",
       "      <td>-0.348665</td>\n",
       "      <td>-1.539362</td>\n",
       "      <td>-0.886158</td>\n",
       "      <td>-1.551371</td>\n",
       "      <td>-1.321760</td>\n",
       "      <td>-0.604779</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.849182</td>\n",
       "      <td>-1.445182</td>\n",
       "      <td>-0.383822</td>\n",
       "      <td>-0.756846</td>\n",
       "      <td>-0.653811</td>\n",
       "      <td>-1.151659</td>\n",
       "      <td>-0.568418</td>\n",
       "      <td>-1.361440</td>\n",
       "      <td>-1.190755</td>\n",
       "      <td>-0.449439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>632</th>\n",
       "      <td>-1.118059</td>\n",
       "      <td>-1.664593</td>\n",
       "      <td>-0.254655</td>\n",
       "      <td>-1.734416</td>\n",
       "      <td>-0.217517</td>\n",
       "      <td>-1.656709</td>\n",
       "      <td>-0.855070</td>\n",
       "      <td>-1.670707</td>\n",
       "      <td>-1.338927</td>\n",
       "      <td>-0.449439</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.665696</td>\n",
       "      <td>-1.388039</td>\n",
       "      <td>-0.955250</td>\n",
       "      <td>-1.190179</td>\n",
       "      <td>-0.946914</td>\n",
       "      <td>-1.113923</td>\n",
       "      <td>-0.738631</td>\n",
       "      <td>-1.278107</td>\n",
       "      <td>-1.489820</td>\n",
       "      <td>-0.342643</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>633 rows × 70 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Age         W         L        PW        PL       MOV       SOS  \\\n",
       "0   -0.623435 -0.289593 -1.697278 -0.422940 -1.528993 -0.505834 -1.041598   \n",
       "1   -1.096553 -0.570843 -1.402196 -0.422940 -1.528993 -0.482511 -1.010510   \n",
       "2   -0.494403 -0.414593 -1.566131 -0.455727 -1.496206 -0.556126 -1.704811   \n",
       "3   -1.096553 -0.414593 -1.566131 -0.455727 -1.496206 -0.539362 -0.834344   \n",
       "4   -0.946016 -0.602093 -1.369409 -0.554088 -1.397845 -0.636301 -1.777349   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "628 -1.505156 -1.508343 -0.418590 -1.537695 -0.414239 -1.454814 -1.041598   \n",
       "629 -1.569672 -1.570843 -0.353016 -1.537695 -0.414239 -1.457729 -0.989785   \n",
       "630 -1.720209 -1.727093 -0.189081 -1.537695 -0.414239 -1.483240 -0.305847   \n",
       "631 -1.483650 -1.539593 -0.385803 -1.603268 -0.348665 -1.539362 -0.886158   \n",
       "632 -1.118059 -1.664593 -0.254655 -1.734416 -0.217517 -1.656709 -0.855070   \n",
       "\n",
       "          SRS      ORtg      DRtg  ...   opp_FT%   opp_ORB   opp_DRB  \\\n",
       "0   -0.490948 -0.326051 -0.915459  ... -1.179457 -0.759467 -1.485863   \n",
       "1   -0.464513 -0.403305 -0.993129  ... -0.794136 -0.959467 -1.526679   \n",
       "2   -0.591402 -0.677983 -1.255265  ... -0.702393 -0.816610 -1.853210   \n",
       "3   -0.510586 -0.626481 -1.167886  ... -0.702393 -1.216610 -1.832801   \n",
       "4   -0.680526 -0.368970 -0.779536  ... -0.812485 -0.816610 -1.832801   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "628 -1.474332 -1.261674 -0.633905  ... -0.537255 -0.959467 -0.669536   \n",
       "629 -1.474332 -1.004163 -0.342643  ... -0.500558 -1.502325 -0.894026   \n",
       "630 -1.450918 -1.167253 -0.517401  ... -0.720741 -1.445182 -0.689944   \n",
       "631 -1.551371 -1.321760 -0.604779  ... -0.849182 -1.445182 -0.383822   \n",
       "632 -1.670707 -1.338927 -0.449439  ... -0.665696 -1.388039 -0.955250   \n",
       "\n",
       "      opp_TRB   opp_AST   opp_STL   opp_BLK   opp_TOV    opp_PF   opp_PTS  \n",
       "0   -1.273512 -0.964155 -0.623357 -1.291822 -0.819774 -1.377670 -1.264973  \n",
       "1   -1.423512 -1.205535 -1.264866 -1.632248 -1.048940 -0.480474 -1.255265  \n",
       "2   -1.590179 -0.929673 -0.661093 -1.121609 -0.736440 -1.022530 -1.187303  \n",
       "3   -1.806846 -0.998638 -1.227130 -1.121609 -0.715607 -0.854306 -1.167886  \n",
       "4   -1.573512 -1.033121 -0.698829 -0.866290 -1.132274 -0.947764 -1.070799  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "628 -0.706846 -0.653811 -0.962980 -1.376929 -0.861440 -1.844960 -0.556236  \n",
       "629 -1.206846 -0.722776 -0.510149 -0.908843 -1.278107 -1.415054 -0.517401  \n",
       "630 -0.990179 -0.653811 -1.302602 -1.079056 -1.111440 -1.545895 -0.459148  \n",
       "631 -0.756846 -0.653811 -1.151659 -0.568418 -1.361440 -1.190755 -0.449439  \n",
       "632 -1.190179 -0.946914 -1.113923 -0.738631 -1.278107 -1.489820 -0.342643  \n",
       "\n",
       "[633 rows x 70 columns]"
      ]
     },
     "execution_count": 503,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the features and the target\n",
    "X = data.drop(columns=['TEAM', 'PLAYOFF_WINS'])\n",
    "for col in X.columns:\n",
    "    col_mean = X[col].mean()\n",
    "    col_min = X[col].min()\n",
    "    col_max = X[col].max()\n",
    "    X[col] = (X[col] - col_mean) / (col_max - col_min)\n",
    "    X[col] = X[col] * 2 - 1\n",
    "y = data['PLAYOFF_WINS']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-44 {color: black;background-color: white;}#sk-container-id-44 pre{padding: 0;}#sk-container-id-44 div.sk-toggleable {background-color: white;}#sk-container-id-44 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-44 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-44 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-44 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-44 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-44 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-44 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-44 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-44 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-44 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-44 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-44 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-44 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-44 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-44 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-44 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-44 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-44 div.sk-item {position: relative;z-index: 1;}#sk-container-id-44 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-44 div.sk-item::before, #sk-container-id-44 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-44 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-44 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-44 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-44 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-44 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-44 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-44 div.sk-label-container {text-align: center;}#sk-container-id-44 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-44 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-44\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-44\" type=\"checkbox\" checked><label for=\"sk-estimator-id-44\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 504,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create and fit the model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error:  12.451385760870506\n",
      "R-squared:  0.4671714708097947\n"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model using mean squared error and R-squared\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Mean Squared Error: \", mse)\n",
    "print(\"R-squared: \", r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NBAPlayoffPredictor(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(NBAPlayoffPredictor, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.bn3 = nn.BatchNorm1d(hidden_size)\n",
    "        self.fc4 = nn.Linear(hidden_size, output_size)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.bn3(self.fc3(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc4(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the features and the target\n",
    "X = data.drop(columns=['TEAM', 'PLAYOFF_WINS']).values\n",
    "y = data['PLAYOFF_WINS'].values\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "# Create DataLoader objects\n",
    "train_data = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "\n",
    "test_data = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(test_data, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Validation Loss: 21.9712\n",
      "Epoch 2, Validation Loss: 21.4791\n",
      "Epoch 3, Validation Loss: 21.1435\n",
      "Epoch 4, Validation Loss: 20.9401\n",
      "Epoch 5, Validation Loss: 20.7126\n",
      "Epoch 6, Validation Loss: 20.5004\n",
      "Epoch 7, Validation Loss: 20.3088\n",
      "Epoch 8, Validation Loss: 20.0817\n",
      "Epoch 9, Validation Loss: 19.9025\n",
      "Epoch 10, Validation Loss: 19.5622\n",
      "Epoch 11, Validation Loss: 19.3794\n",
      "Epoch 12, Validation Loss: 19.1005\n",
      "Epoch 13, Validation Loss: 18.8526\n",
      "Epoch 14, Validation Loss: 18.3993\n",
      "Epoch 15, Validation Loss: 17.8087\n",
      "Epoch 16, Validation Loss: 17.6963\n",
      "Epoch 17, Validation Loss: 17.4397\n",
      "Epoch 18, Validation Loss: 17.2271\n",
      "Epoch 19, Validation Loss: 16.8497\n",
      "Epoch 20, Validation Loss: 16.5290\n",
      "Epoch 21, Validation Loss: 16.2715\n",
      "Epoch 22, Validation Loss: 15.7645\n",
      "Epoch 23, Validation Loss: 15.3746\n",
      "Epoch 24, Validation Loss: 14.8867\n",
      "Epoch 25, Validation Loss: 14.9500\n",
      "Epoch 26, Validation Loss: 14.6404\n",
      "Epoch 27, Validation Loss: 13.6266\n",
      "Epoch 28, Validation Loss: 13.8545\n",
      "Epoch 29, Validation Loss: 13.6680\n",
      "Epoch 30, Validation Loss: 13.2961\n",
      "Epoch 31, Validation Loss: 13.0671\n",
      "Epoch 32, Validation Loss: 12.7020\n",
      "Epoch 33, Validation Loss: 12.5564\n",
      "Epoch 34, Validation Loss: 12.3372\n",
      "Epoch 35, Validation Loss: 12.2906\n",
      "Epoch 36, Validation Loss: 11.8825\n",
      "Epoch 37, Validation Loss: 11.6015\n",
      "Epoch 38, Validation Loss: 11.6403\n",
      "Epoch 39, Validation Loss: 11.2218\n",
      "Epoch 40, Validation Loss: 11.3357\n",
      "Epoch 41, Validation Loss: 10.7195\n",
      "Epoch 42, Validation Loss: 10.3149\n",
      "Epoch 43, Validation Loss: 10.6938\n",
      "Epoch 44, Validation Loss: 10.3197\n",
      "Epoch 45, Validation Loss: 10.0228\n",
      "Epoch 46, Validation Loss: 9.8995\n",
      "Epoch 47, Validation Loss: 10.2458\n",
      "Epoch 48, Validation Loss: 10.4512\n",
      "Epoch 49, Validation Loss: 10.2258\n",
      "Epoch 50, Validation Loss: 9.6720\n",
      "Epoch 51, Validation Loss: 9.1782\n",
      "Epoch 52, Validation Loss: 9.5772\n",
      "Epoch 53, Validation Loss: 9.7387\n",
      "Epoch 54, Validation Loss: 9.7667\n",
      "Epoch 55, Validation Loss: 8.9534\n",
      "Epoch 56, Validation Loss: 9.4098\n",
      "Epoch 57, Validation Loss: 9.0494\n",
      "Epoch 58, Validation Loss: 9.1778\n",
      "Epoch 59, Validation Loss: 9.2548\n",
      "Epoch 60, Validation Loss: 9.0720\n",
      "Epoch 61, Validation Loss: 8.7299\n",
      "Epoch 62, Validation Loss: 8.8714\n",
      "Epoch 63, Validation Loss: 8.7227\n",
      "Epoch 64, Validation Loss: 8.8238\n",
      "Epoch 65, Validation Loss: 8.5900\n",
      "Epoch 66, Validation Loss: 8.6202\n",
      "Epoch 67, Validation Loss: 8.4802\n",
      "Epoch 68, Validation Loss: 8.5107\n",
      "Epoch 69, Validation Loss: 8.2467\n",
      "Epoch 70, Validation Loss: 8.3309\n",
      "Epoch 71, Validation Loss: 8.4835\n",
      "Epoch 72, Validation Loss: 8.5077\n",
      "Epoch 73, Validation Loss: 8.2792\n",
      "Epoch 74, Validation Loss: 8.2696\n",
      "Epoch 75, Validation Loss: 8.3219\n",
      "Epoch 76, Validation Loss: 8.4705\n",
      "Epoch 77, Validation Loss: 8.5432\n",
      "Epoch 78, Validation Loss: 8.5389\n",
      "Epoch 79, Validation Loss: 8.3836\n",
      "Epoch 80, Validation Loss: 8.2243\n",
      "Epoch 81, Validation Loss: 8.2278\n",
      "Epoch 82, Validation Loss: 8.2866\n",
      "Epoch 83, Validation Loss: 8.5129\n",
      "Epoch 84, Validation Loss: 8.3179\n",
      "Epoch 85, Validation Loss: 8.4559\n",
      "Epoch 86, Validation Loss: 8.3835\n",
      "Epoch 87, Validation Loss: 8.3167\n",
      "Epoch 88, Validation Loss: 8.1473\n",
      "Epoch 89, Validation Loss: 8.2504\n",
      "Epoch 90, Validation Loss: 8.1833\n",
      "Epoch 91, Validation Loss: 8.1574\n",
      "Epoch 92, Validation Loss: 8.1949\n",
      "Epoch 93, Validation Loss: 8.1167\n",
      "Epoch 94, Validation Loss: 8.1710\n",
      "Epoch 95, Validation Loss: 8.1264\n",
      "Epoch 96, Validation Loss: 8.0446\n",
      "Epoch 97, Validation Loss: 8.1857\n",
      "Epoch 98, Validation Loss: 8.1062\n",
      "Epoch 99, Validation Loss: 8.1107\n",
      "Epoch 100, Validation Loss: 8.1880\n",
      "Epoch 101, Validation Loss: 8.1587\n",
      "Epoch 102, Validation Loss: 8.0689\n",
      "Epoch 103, Validation Loss: 8.1265\n",
      "Epoch 104, Validation Loss: 8.0558\n",
      "Epoch 105, Validation Loss: 8.0305\n",
      "Epoch 106, Validation Loss: 7.9080\n",
      "Epoch 107, Validation Loss: 8.1577\n",
      "Epoch 108, Validation Loss: 7.9499\n",
      "Epoch 109, Validation Loss: 8.1023\n",
      "Epoch 110, Validation Loss: 8.1012\n",
      "Epoch 111, Validation Loss: 8.0758\n",
      "Epoch 112, Validation Loss: 8.1540\n",
      "Epoch 113, Validation Loss: 8.0115\n",
      "Epoch 114, Validation Loss: 8.0627\n",
      "Epoch 115, Validation Loss: 8.1655\n",
      "Epoch 116, Validation Loss: 8.0428\n",
      "Epoch 117, Validation Loss: 8.0785\n",
      "Epoch 118, Validation Loss: 8.0341\n",
      "Epoch 119, Validation Loss: 8.1107\n",
      "Epoch 120, Validation Loss: 8.0328\n",
      "Epoch 121, Validation Loss: 8.0321\n",
      "Epoch 122, Validation Loss: 7.9992\n",
      "Epoch 123, Validation Loss: 8.0148\n",
      "Epoch 124, Validation Loss: 8.0058\n",
      "Epoch 125, Validation Loss: 7.9680\n",
      "Epoch 126, Validation Loss: 7.9853\n",
      "Epoch 127, Validation Loss: 7.9877\n",
      "Epoch 128, Validation Loss: 8.0018\n",
      "Epoch 129, Validation Loss: 8.0239\n",
      "Epoch 130, Validation Loss: 8.0941\n",
      "Epoch 131, Validation Loss: 8.0276\n",
      "Epoch 132, Validation Loss: 8.0382\n",
      "Epoch 133, Validation Loss: 8.1068\n",
      "Epoch 134, Validation Loss: 8.0410\n",
      "Epoch 135, Validation Loss: 7.9742\n",
      "Epoch 136, Validation Loss: 8.0350\n",
      "Epoch 137, Validation Loss: 8.0291\n",
      "Epoch 138, Validation Loss: 7.9990\n",
      "Epoch 139, Validation Loss: 7.9874\n",
      "Epoch 140, Validation Loss: 7.9533\n",
      "Epoch 141, Validation Loss: 7.9933\n",
      "Epoch 142, Validation Loss: 7.9683\n",
      "Epoch 143, Validation Loss: 7.9669\n",
      "Epoch 144, Validation Loss: 7.9893\n",
      "Epoch 145, Validation Loss: 7.9749\n",
      "Epoch 146, Validation Loss: 7.9496\n",
      "Epoch 147, Validation Loss: 7.9718\n",
      "Epoch 148, Validation Loss: 8.0022\n",
      "Epoch 149, Validation Loss: 7.9819\n",
      "Epoch 150, Validation Loss: 8.0204\n",
      "Epoch 151, Validation Loss: 7.9898\n",
      "Epoch 152, Validation Loss: 7.9952\n",
      "Epoch 153, Validation Loss: 8.0619\n",
      "Epoch 154, Validation Loss: 8.0306\n",
      "Epoch 155, Validation Loss: 8.0793\n",
      "Epoch 156, Validation Loss: 8.1211\n",
      "Epoch 157, Validation Loss: 8.0711\n",
      "Epoch 158, Validation Loss: 8.1019\n",
      "Epoch 159, Validation Loss: 8.0516\n",
      "Epoch 160, Validation Loss: 8.1404\n",
      "Epoch 161, Validation Loss: 8.0605\n",
      "Epoch 162, Validation Loss: 8.0367\n",
      "Epoch 163, Validation Loss: 8.0824\n",
      "Epoch 164, Validation Loss: 8.1251\n",
      "Epoch 165, Validation Loss: 8.2132\n",
      "Epoch 166, Validation Loss: 8.1057\n",
      "Epoch 167, Validation Loss: 8.0937\n",
      "Epoch 168, Validation Loss: 8.1543\n",
      "Epoch 169, Validation Loss: 8.1990\n",
      "Epoch 170, Validation Loss: 8.0844\n",
      "Epoch 171, Validation Loss: 8.0252\n",
      "Epoch 172, Validation Loss: 8.1365\n",
      "Epoch 173, Validation Loss: 8.1513\n",
      "Epoch 174, Validation Loss: 8.0627\n",
      "Epoch 175, Validation Loss: 8.0738\n",
      "Epoch 176, Validation Loss: 8.1720\n",
      "Epoch 177, Validation Loss: 8.0821\n",
      "Epoch 178, Validation Loss: 8.0916\n",
      "Epoch 179, Validation Loss: 8.0734\n",
      "Epoch 180, Validation Loss: 8.1804\n",
      "Epoch 181, Validation Loss: 8.1848\n",
      "Epoch 182, Validation Loss: 8.0508\n",
      "Epoch 183, Validation Loss: 8.1605\n",
      "Epoch 184, Validation Loss: 8.1398\n",
      "Epoch 185, Validation Loss: 8.1098\n",
      "Epoch 186, Validation Loss: 8.0783\n",
      "Epoch 187, Validation Loss: 8.0626\n",
      "Epoch 188, Validation Loss: 8.0547\n",
      "Epoch 189, Validation Loss: 8.1095\n",
      "Epoch 190, Validation Loss: 8.0858\n",
      "Epoch 191, Validation Loss: 8.0711\n",
      "Epoch 192, Validation Loss: 8.0800\n",
      "Epoch 193, Validation Loss: 8.0941\n",
      "Epoch 194, Validation Loss: 7.9900\n",
      "Epoch 195, Validation Loss: 8.0536\n",
      "Epoch 196, Validation Loss: 8.0689\n",
      "Epoch 197, Validation Loss: 8.0702\n",
      "Epoch 198, Validation Loss: 8.0130\n",
      "Epoch 199, Validation Loss: 7.9876\n",
      "Epoch 200, Validation Loss: 8.0400\n",
      "Epoch 201, Validation Loss: 8.0520\n",
      "Epoch 202, Validation Loss: 8.0861\n",
      "Epoch 203, Validation Loss: 8.0745\n",
      "Epoch 204, Validation Loss: 8.0852\n",
      "Epoch 205, Validation Loss: 8.1687\n",
      "Epoch 206, Validation Loss: 8.1058\n",
      "Early stopping triggered. Stopping training at epoch 206.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model, loss function, and optimizer\n",
    "input_size = X_train.shape[1]\n",
    "hidden_size = 64\n",
    "output_size = 1\n",
    "model = NBAPlayoffPredictor(input_size, hidden_size, output_size)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# Train the model\n",
    "patience = 100\n",
    "early_stop_counter = 0\n",
    "best_val_loss = float('inf')\n",
    "epochs = 1000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Training loop\n",
    "    model.train()\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to('cpu'), targets.to('cpu')\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to('cpu'), targets.to('cpu')\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    val_loss /= len(test_loader)\n",
    "    print(f\"Epoch {epoch+1}, Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "    # Early stopping logic\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        early_stop_counter = 0\n",
    "    else:\n",
    "        early_stop_counter += 1\n",
    "\n",
    "    if early_stop_counter >= patience:\n",
    "        print(f\"Early stopping triggered. Stopping training at epoch {epoch+1}.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_loss = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 8.1058\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.eval()\n",
    "\n",
    "# Initialize a variable to store the test loss\n",
    "test_loss = 0\n",
    "\n",
    "# No need to compute gradients during evaluation, so use torch.no_grad()\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in test_loader:\n",
    "        # Move inputs and targets to the appropriate device (CPU or GPU)\n",
    "        inputs, targets = inputs.to('cpu'), targets.to('cpu')\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # Accumulate the test loss\n",
    "        test_loss += loss.item()\n",
    "\n",
    "# Compute the average test loss\n",
    "average_test_loss = test_loss / len(test_loader)\n",
    "\n",
    "# Print the average test loss\n",
    "print(\"Test Loss: {:.4f}\".format(average_test_loss))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
